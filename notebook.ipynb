{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3d001b0-2e2f-4b58-8442-99520bad831f",
   "metadata": {},
   "source": [
    "# Sowing Success: How Machine Learning Helps Farmers Select the Best Crops\n",
    "\n",
    "![Farmer in a field](farmer_in_a_field.jpg)\n",
    "\n",
    "Measuring essential soil metrics such as nitrogen, phosphorous, potassium levels, and pH value is an important aspect of assessing soil condition. However, it can be an expensive and time-consuming process, which can cause farmers to prioritize which metrics to measure based on their budget constraints.\n",
    "\n",
    "Farmers have various options when it comes to deciding which crop to plant each season. Their primary objective is to maximize the yield of their crops, taking into account different factors. One crucial factor that affects crop growth is the condition of the soil in the field, which can be assessed by measuring basic elements such as nitrogen and potassium levels. Each crop has an ideal soil condition that ensures optimal growth and maximum yield.\n",
    "\n",
    "A farmer reached out to you as a machine learning expert for assistance in selecting the best crop for his field. They've provided you with a dataset called `soil_measures.csv`, which contains:\n",
    "\n",
    "- `\"N\"`: Nitrogen content ratio in the soil\n",
    "- `\"P\"`: Phosphorous content ratio in the soil\n",
    "- `\"K\"`: Potassium content ratio in the soil\n",
    "- `\"pH\"` value of the soil\n",
    "- `\"crop\"`: categorical values that contain various crops (target variable).\n",
    "\n",
    "Each row in this dataset represents various measures of the soil in a particular field. Based on these measurements, the crop specified in the `\"crop\"` column is the optimal choice for that field.  \n",
    "\n",
    "In this project, you will build multi-class classification models to predict the type of `\"crop\"` and identify the single most importance feature for predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fdb38b-d428-4659-aea4-b12037ca07e2",
   "metadata": {},
   "source": [
    "### Loading and Preparing the Data\n",
    "\n",
    "Before diving into the analysis, we will first import the necessary libraries for data processing, visualisation, and modelling. We will then load the dataset, which contains soil measurements and the corresponding ideal crop for each field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0eb4f16-5a99-460d-a5ba-706b7ef0bbe7",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 14,
    "id": "bA5ajAmk7XH6",
    "jupyter": {
     "source_hidden": true
    },
    "lastExecutedAt": 1707317470036,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# All required libraries are imported here for you.\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n# Load the dataset\ncrops = pd.read_csv(\"soil_measures.csv\")\n\n# Write your code here"
   },
   "outputs": [],
   "source": [
    "# All required libraries are imported here for you.\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "crops = pd.read_csv(\"soil_measures.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe3611-dfac-49e9-a50c-4eea9cb31629",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "To understand the patterns in the soil data and identify any useful trends, we begin with a visual and statistical exploration. This helps inform how we approach the predictive modelling task ahead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d6991-7cb3-4a54-bd5e-ed3b5e8f28aa",
   "metadata": {},
   "source": [
    "### First Look at the Data\n",
    "\n",
    "Before we begin building any models, it's helpful to take a quick look at the dataset. Here, we display the first few rows to get familiar with the structure and confirm the presence of the expected features—soil nutrients (`N`, `P`, `K`), pH level, and the target variable `crop`, which identifies the optimal crop for each soil composition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b9eb02-af90-4149-b464-30be65fd4c6a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>ph</th>\n",
       "      <th>crop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>6.502985</td>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>58</td>\n",
       "      <td>41</td>\n",
       "      <td>7.038096</td>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>55</td>\n",
       "      <td>44</td>\n",
       "      <td>7.840207</td>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "      <td>6.980401</td>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>7.628473</td>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    N   P   K        ph  crop\n",
       "0  90  42  43  6.502985  rice\n",
       "1  85  58  41  7.038096  rice\n",
       "2  60  55  44  7.840207  rice\n",
       "3  74  35  40  6.980401  rice\n",
       "4  78  42  42  7.628473  rice"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crops.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22d2844f-a9a6-4b3b-86d5-b5106c84bdf6",
   "metadata": {},
   "source": [
    "From this preview, we can see that the dataset is clean and clearly structured, with consistent formatting across columns. Each row corresponds to a different set of soil conditions and a suitable crop.\n",
    "\n",
    "Next, we examine how frequently each crop type appears in the dataset. This helps us confirm whether the classes are balanced or if any particular crops are over- or under-represented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b98c057-0c55-4fbc-a3f9-10e3565b9464",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crop\n",
       "apple          100\n",
       "banana         100\n",
       "blackgram      100\n",
       "chickpea       100\n",
       "coconut        100\n",
       "coffee         100\n",
       "cotton         100\n",
       "grapes         100\n",
       "jute           100\n",
       "kidneybeans    100\n",
       "lentil         100\n",
       "maize          100\n",
       "mango          100\n",
       "mothbeans      100\n",
       "mungbean       100\n",
       "muskmelon      100\n",
       "orange         100\n",
       "papaya         100\n",
       "pigeonpeas     100\n",
       "pomegranate    100\n",
       "rice           100\n",
       "watermelon     100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crops['crop'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c316dcb-1fc6-4a38-b2ff-6a644cac47de",
   "metadata": {},
   "source": [
    "We can now inspect the structure of the dataset in more detail using `.info()`. This will show us the number of entries, column types, and whether any data is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2212a41-1ed3-447f-92fe-b0ba99991ed9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2200 entries, 0 to 2199\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   N       2200 non-null   int64  \n",
      " 1   P       2200 non-null   int64  \n",
      " 2   K       2200 non-null   int64  \n",
      " 3   ph      2200 non-null   float64\n",
      " 4   crop    2200 non-null   object \n",
      "dtypes: float64(1), int64(3), object(1)\n",
      "memory usage: 86.1+ KB\n"
     ]
    }
   ],
   "source": [
    "crops.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93630740-bdbf-45ee-b612-252d9efb31c6",
   "metadata": {},
   "source": [
    "All 2,200 entries are complete with **no missing values**. Each column has a consistent data type—ideal for a machine learning workflow. This means we won’t need to worry about data cleaning or imputation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f675317e-d28e-4142-b62d-e2a9e1120416",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>ph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>50.551818</td>\n",
       "      <td>53.362727</td>\n",
       "      <td>48.149091</td>\n",
       "      <td>6.469480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>36.917334</td>\n",
       "      <td>32.985883</td>\n",
       "      <td>50.647931</td>\n",
       "      <td>0.773938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.504752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>5.971693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>6.425045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>84.250000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>6.923643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>140.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>9.935091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 N            P            K           ph\n",
       "count  2200.000000  2200.000000  2200.000000  2200.000000\n",
       "mean     50.551818    53.362727    48.149091     6.469480\n",
       "std      36.917334    32.985883    50.647931     0.773938\n",
       "min       0.000000     5.000000     5.000000     3.504752\n",
       "25%      21.000000    28.000000    20.000000     5.971693\n",
       "50%      37.000000    51.000000    32.000000     6.425045\n",
       "75%      84.250000    68.000000    49.000000     6.923643\n",
       "max     140.000000   145.000000   205.000000     9.935091"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crops.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b396ca45-850e-4913-a64c-b597f6e21dc6",
   "metadata": {},
   "source": [
    "The descriptive statistics reveal quite a bit about the soil conditions in the dataset:\n",
    "\n",
    "- Nutrient levels (N, P, K) vary significantly—particularly potassium, which ranges from 5 to 205—indicating diverse soil fertility across samples.\n",
    "- The pH values span from strongly acidic (3.5) to highly alkaline (almost 10), though the average is around 6.47, which is near-neutral and suitable for most crops.\n",
    "- The relatively high standard deviations, especially for potassium and nitrogen, suggest meaningful variation in soil profiles—useful for distinguishing between crop preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58beca35-e3a9-4532-a3fb-e2c27f98f395",
   "metadata": {},
   "source": [
    "## Preparing the Data for Modeling\n",
    "\n",
    "Before training the model, the dataset must be prepared for machine learning. Key preprocessing steps include:\n",
    "\n",
    "- **Reducing memory usage** by converting the categorical `crop` column to a more efficient data type.\n",
    "- **Encoding the target variable** so it can be used in classification models.\n",
    "- **Scaling numerical features** to ensure consistency across different ranges of values.\n",
    "\n",
    "These steps help improve both model performance and computational efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5592820c-7f1d-42b8-af30-2bc40f409213",
   "metadata": {},
   "source": [
    "### Optimising memory usage\n",
    "\n",
    "Converting the `crop` column from object to category type resulted in a substantial reduction in memory usage. This is an effective preprocessing step when working with repeated string values, especially in larger datasets where efficiency matters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1388049c-958e-4b5d-83eb-b4f874d23f2b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory usage with \"object\" type: 189.5 kB\n",
      "memory usage with \"category\" type: 72.8 kB\n"
     ]
    }
   ],
   "source": [
    "memory_used_obj = crops.memory_usage(deep=True).sum() / 1024  # in kB\n",
    "crops['crop'] = crops['crop'].astype('category')\n",
    "memory_used_cat = crops.memory_usage(deep=True).sum() / 1024  # in kB\n",
    "print(f'memory usage with \"object\" type: {memory_used_obj.round(1)} kB')\n",
    "print(f'memory usage with \"category\" type: {memory_used_cat.round(1)} kB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62abe23a-417a-46e7-9c84-d4aed1b4665d",
   "metadata": {},
   "source": [
    "### Splitting Features and Target\n",
    "\n",
    "In this step, we separate the dataset into input features and the target variable. The feature matrix, `X`, will contain the soil measurements, while the target vector, `y`, will store the corresponding crop labels.\n",
    "\n",
    "After reviewing the distribution of the values, we now proceed to extract the features (`N`, `P`, `K`, and `pH`) into a NumPy array, `X`, and store the target variable (`crop`) separately in `y`. This separation is a standard preprocessing step that ensures compatibility with machine learning models and prepares the data for subsequent transformations or model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95aa3b56-d881-478b-8b29-df55a1e7f6d4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X = crops.drop('crop', axis=1).values \n",
    "y = np.array(crops['crop'].values) # 'values' is used to get only the numpy array.\n",
    "print(type(X), type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da601b08-ea37-4446-97e0-48bb21627981",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[90.         42.         43.          6.50298529]\n",
      " [85.         58.         41.          7.03809636]\n",
      " [60.         55.         44.          7.84020714]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X[:3])\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db1c6556-83d7-4ca1-8e5b-38d6b6f4d537",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rice' 'rice' 'rice']\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(y[:3])\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c76fa7-78ec-4c26-9cfd-3b3288de0f35",
   "metadata": {},
   "source": [
    "### Splitting the data for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9264cd9-66e7-4e87-979a-a291cccdb469",
   "metadata": {},
   "source": [
    "To ensure the model generalises well, it’s essential to split the dataset into training and testing subsets. Here, we use the train_test_split function to divide the data, reserving 20% for testing. A random seed is set to ensure reproducibility of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28715e82-7910-4068-a438-496752cc4a2a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb8486-da13-45b2-b0cf-a7ec6d1a8842",
   "metadata": {},
   "source": [
    "### Feature Scaling: Enhancing Model Performance\n",
    "\n",
    "To ensure the performance and stability of the machine learning models, we scale the features to standardise their range. Using `StandardScaler`, the data is transformed to have a mean of zero and a standard deviation of one. This step is particularly important for models sensitive to the magnitude of the data, such as gradient-based models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cec5ad2-5ff0-4eef-85a7-a2015cd2a097",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c51c870-d99f-4636-9bd9-acee108a7e6e",
   "metadata": {},
   "source": [
    "## Evaluating the Contribution of Each Feature\n",
    "\n",
    "Next, we evaluate the performance of a **logistic regression** model using each individual feature. By training the model on each feature independently, we can assess the contribution of each one to the overall prediction accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0b7d067-dcc3-4491-950d-fc55b93b094d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "features = [\"N\", \"P\", \"K\", \"ph\"]\n",
    "features_n = range(0,4)\n",
    "\n",
    "# Store mean accuracy for each feature\n",
    "feature_scores = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf096c-66b7-4943-8e10-2f015fb7b93a",
   "metadata": {},
   "source": [
    "The following code trains a **logistic regression** model on each individual feature and calculates the **F1 score** for each. The results will provide insight into which features are more predictive of the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5103d9cf-5f38-430a-9ced-ebef3fbd2b64",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Train logistic regression on each individual feature\n",
    "for i in features_n:\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train_scaled[:,i].reshape(-1,1), y_train) \n",
    "    y_pred = logreg.predict(X_test_scaled[:,i].reshape(-1,1)) # calculate the predicted values\n",
    "    score = metrics.f1_score(y_test, y_pred, average='weighted') # comparing the test vs predicted values\n",
    "    feature_scores[features[i]] = score # create the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069095c4-ca87-4654-8be7-00be74e30a23",
   "metadata": {},
   "source": [
    "### F1 Scores for Each Feature\n",
    "\n",
    "The table below displays the F1 score for each feature used in the model. These scores provide a measure of the model's performance based on individual features. A higher score indicates that the feature contributes more effectively to accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d757db9-4f7f-4994-b26c-af8914ae2fdb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for \"N\": 0.0712\n",
      "F1 score for \"P\": 0.1366\n",
      "F1 score for \"K\": 0.1381\n",
      "F1 score for \"ph\": 0.0279\n"
     ]
    }
   ],
   "source": [
    "# Display the scores\n",
    "for feature, score in feature_scores.items():\n",
    "    print(f'F1 score for \"{feature}\": {score:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a6ef64-6a4f-4a64-b49a-e6ce52745c95",
   "metadata": {},
   "source": [
    "### Identifying the Best Predictive Feature\n",
    "\n",
    "The following code identifies the feature with the highest predictive power by calculating the F1 score for each feature and selecting the one with the highest value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f7adae4-4115-4c25-9def-07e12086bff6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "best_predictive_feature = {\n",
    "    max(feature_scores, key=feature_scores.get): max(feature_scores.values())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5515b2-6589-4b61-b8dc-046c7d545483",
   "metadata": {},
   "source": [
    "The output reveals the feature that offers the most predictive power. This feature has the highest F1 score, indicating it is the most effective in contributing to the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7e44742-6066-489f-98b3-5d97bcfdf8b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best predicted feature is K with a value of 0.1381\n"
     ]
    }
   ],
   "source": [
    "print(f\"The best predicted feature is {list(best_predictive_feature.keys())[0]} with a value of {list(best_predictive_feature.values())[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca146d52-f8d0-4317-9525-da23aaa3c91c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (Data Analysis Base)",
   "language": "python",
   "name": "data_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
